{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeaCarop/Conv_Net/blob/main/Parte3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYp4p95zIjzj"
      },
      "source": [
        "**## Parte 3: Clasificación de imágenes de gatos vs perros**\n",
        "\n",
        "En este ejercicio, veremos dos técnicas para reutilizar datos de características generados a partir de modelos de imágenes que ya han sido entrenados en grandes conjuntos de datos, extracción de características y ajuste fino, y los usaremos para mejorar la precisión del modelo de clasificación de gatos y perros. Ya que, en la parte 3 se aumentó el accuracy entorno al 75%, pero sigue habiendo un 25% de error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThTNn3uRKZ2z"
      },
      "source": [
        "Voy a utilizar el modelo Inception V3 desarrollado en Google y entrenado previamente en ImageNet, un gran conjunto de datos de imágenes web (1,4 millones de imágenes y 1000 clases). \n",
        "\n",
        "Primero, debemos elegiré qué capa intermedia de Inception V3 usaré para la extracción de características. Una práctica común es usar la salida de la última capa antes de la operación Flatten, la llamada \"capa de cuello de botella\". El razonamiento aquí es que las siguientes capas completamente conectadas serán demasiado especializadas para la tarea en la que se entrenó la red y, por lo tanto, las funciones aprendidas por estas capas no serán muy útiles para una nueva tarea. Las características de cuello de botella, sin embargo, conservan mucha generalidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHT5kt8bIhJb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le1etgVMJOv5",
        "outputId": "fd67193a-7829-45b3-d815-37a58f7cfbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-18 13:13:02--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.69.128, 173.194.79.128, 108.177.119.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.69.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M  44.9MB/s    in 1.9s    \n",
            "\n",
            "2022-03-18 13:13:04 (44.9 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf_NG1eBJSLp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "pre_trained_model = InceptionV3(\n",
        "    input_shape=(150, 150, 3), include_top=False, weights=None)\n",
        "pre_trained_model.load_weights(local_weights_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K1YoBuxJf7m"
      },
      "outputs": [],
      "source": [
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqmOHptRJpUq"
      },
      "source": [
        "\n",
        "La capa que usaré para la extracción de características en la Inception V3 se llama mixed7. No es el cuello de botella de la red, pero lo estamos usando para mantener un mapa de características suficientemente grande (7x7 en este caso). (El uso de la capa de cuello de botella habría dado como resultado un mapa de características de 3x3, que es un poco pequeño). Obtengamos el resultado de mixed7:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g50mT8IJvYH",
        "outputId": "aa778ede-56c3-489c-9391-b0697fd502a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last layer output shape: (None, 7, 7, 768)\n"
          ]
        }
      ],
      "source": [
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape:', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX2dpTMnJ0_3",
        "outputId": "a5872912-cec3-4da8-8652-a8cb8dda5ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "x = layers.Flatten()(last_output)\n",
        "\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "\n",
        "x = layers.Dropout(0.2)(x)\n",
        "\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "model = Model(pre_trained_model.input, x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.0001),\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE2BA2dGK7ne",
        "outputId": "5068133c-1ed2-4d5f-e89d-f9e667a820e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-18 13:13:28--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.126.128, 108.177.127.128, 172.217.218.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   146MB/s    in 0.4s    \n",
            "\n",
            "2022-03-18 13:13:28 (146 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "   https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O \\\n",
        "   /tmp/cats_and_dogs_filtered.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TthWuO8TK-VL",
        "outputId": "57b9ba6e-1b7d-4044-8125-2d9769c7b649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "\n",
        "# Añadir parámetros de data-augmentation a ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# Los datos de validación no deben ser aumentados!!\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir, \n",
        "        target_size=(150, 150),  \n",
        "        batch_size=20,\n",
        "      \n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA4h7lGmLdr1",
        "outputId": "6592fa4f-d007-4d0f-dfc8-6f5dd6f53b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "100/100 - 117s - loss: 0.3534 - acc: 0.8555 - val_loss: 0.1621 - val_acc: 0.9340 - 117s/epoch - 1s/step\n",
            "Epoch 2/2\n",
            "100/100 - 113s - loss: 0.2337 - acc: 0.9110 - val_loss: 0.1002 - val_acc: 0.9650 - 113s/epoch - 1s/step\n"
          ]
        }
      ],
      "source": [
        "# ENTRENAMIENTO\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=2,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pzhh4GUNIut"
      },
      "source": [
        "Hemos mejorado el modelo e comparación a la Parte 2 (Accuracy-0.75%). Ahora tenemos una precisión en validación de 0.91%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbX0EVXCL5Hl"
      },
      "source": [
        "### **Mejorando aún más la precisión con Fine-Turing**\n",
        "\n",
        "En nuestro experimento de extracción de características, solo intentamos agregar dos capas de clasificación encima de una capa de Inception V3. Los pesos de la red preentrenada no se actualizaron durante el entrenamiento. Una forma de aumentar aún más el rendimiento es \"afinar\" los pesos de las capas superiores del modelo preentrenado junto con el entrenamiento del clasificador de nivel superior. Un par de notas importantes sobre Fine-Turing:\n",
        "\n",
        "- Fine-Turing solo debe intentarse después de haber entrenado el clasificador de nivel superior con el modelo preentrenado establecido en no entrenable. Si se agrega un clasificador inicializado aleatoriamente encima de un modelo preentrenado e intentas entrenar todas las capas juntas, la magnitud de las actualizaciones de gradiente serán demasiado grandes (debido a los pesos aleatorios del clasificador) y tu modelo preentrenado simplemente olvidará todo lo que ha aprendido\n",
        "- Además, ajustamos con precisión solo las capas superiores del modelo preentrenado en lugar de todas las capas del modelo preentrenado porque, en una convnet, cuanto más arriba está una capa, más especializada es. Las primeras capas en una convnet aprenden características muy simples y genéricas, que se generalizan a casi todos los tipos de imágenes. Pero a medida que avanza, las funciones son cada vez más específicas para el conjunto de datos en el que se entrena el modelo. El objetivo del Fine-Turing es adaptar estas características especializadas para que funcionen con el nuevo conjunto de datos.\n",
        "\n",
        "Todo lo que hay que hacer para implementar el Fine-Turing es configurar las capas superiores de Inception V3 para que se puedan entrenar, volver a compilar el modelo (necesario para que estos cambios surtan efecto) y reanudar el entrenamiento. Descongelar todas las capas que pertenecen al módulo mixed7, es decir, todas las capas encontradas después de mixed6, y volver a compilar el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKbOngYvNW8r",
        "outputId": "5d9994d5-c1bb-44de-98f2-ac4868bfc45d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "unfreeze = False\n",
        "\n",
        "# Unfreeze todos los modelos despues de \"mixed6\"\n",
        "for layer in pre_trained_model.layers:\n",
        "  if unfreeze:\n",
        "    layer.trainable = True\n",
        "  if layer.name == 'mixed6':\n",
        "    unfreeze = True\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=SGD(\n",
        "                  lr=0.00001, # ratio de aprendizaje muy bajo\n",
        "                  momentum=0.9),\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnIGrx8rNoOP",
        "outputId": "b0b1d93c-fac4-4963-bed5-22eafd323253"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  import sys\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "100/100 - 157s - loss: 0.3165 - acc: 0.8600 - val_loss: 0.1124 - val_acc: 0.9500 - 157s/epoch - 2s/step\n",
            "Epoch 2/50\n",
            "100/100 - 152s - loss: 0.2690 - acc: 0.8845 - val_loss: 0.1142 - val_acc: 0.9560 - 152s/epoch - 2s/step\n",
            "Epoch 3/50\n",
            "100/100 - 152s - loss: 0.2277 - acc: 0.9030 - val_loss: 0.1169 - val_acc: 0.9540 - 152s/epoch - 2s/step\n",
            "Epoch 4/50\n",
            "100/100 - 152s - loss: 0.2264 - acc: 0.9095 - val_loss: 0.1190 - val_acc: 0.9560 - 152s/epoch - 2s/step\n",
            "Epoch 5/50\n",
            "100/100 - 150s - loss: 0.2261 - acc: 0.9050 - val_loss: 0.1201 - val_acc: 0.9530 - 150s/epoch - 1s/step\n",
            "Epoch 6/50\n",
            "100/100 - 150s - loss: 0.2030 - acc: 0.9140 - val_loss: 0.1213 - val_acc: 0.9510 - 150s/epoch - 1s/step\n",
            "Epoch 7/50\n",
            "100/100 - 150s - loss: 0.2139 - acc: 0.9150 - val_loss: 0.1215 - val_acc: 0.9510 - 150s/epoch - 1s/step\n",
            "Epoch 8/50\n",
            "100/100 - 150s - loss: 0.2260 - acc: 0.8985 - val_loss: 0.1205 - val_acc: 0.9520 - 150s/epoch - 1s/step\n",
            "Epoch 9/50\n",
            "100/100 - 150s - loss: 0.1967 - acc: 0.9235 - val_loss: 0.1206 - val_acc: 0.9510 - 150s/epoch - 2s/step\n",
            "Epoch 10/50\n",
            "100/100 - 150s - loss: 0.2070 - acc: 0.9170 - val_loss: 0.1209 - val_acc: 0.9510 - 150s/epoch - 2s/step\n",
            "Epoch 11/50\n",
            "100/100 - 150s - loss: 0.2128 - acc: 0.9105 - val_loss: 0.1199 - val_acc: 0.9510 - 150s/epoch - 1s/step\n",
            "Epoch 12/50\n",
            "100/100 - 150s - loss: 0.2030 - acc: 0.9140 - val_loss: 0.1193 - val_acc: 0.9510 - 150s/epoch - 2s/step\n",
            "Epoch 13/50\n",
            "100/100 - 150s - loss: 0.2089 - acc: 0.9190 - val_loss: 0.1196 - val_acc: 0.9510 - 150s/epoch - 1s/step\n",
            "Epoch 14/50\n",
            "100/100 - 150s - loss: 0.2135 - acc: 0.9105 - val_loss: 0.1195 - val_acc: 0.9490 - 150s/epoch - 2s/step\n",
            "Epoch 15/50\n",
            "100/100 - 150s - loss: 0.1955 - acc: 0.9285 - val_loss: 0.1184 - val_acc: 0.9500 - 150s/epoch - 2s/step\n",
            "Epoch 16/50\n",
            "100/100 - 150s - loss: 0.2033 - acc: 0.9140 - val_loss: 0.1184 - val_acc: 0.9500 - 150s/epoch - 2s/step\n",
            "Epoch 17/50\n",
            "100/100 - 150s - loss: 0.1998 - acc: 0.9175 - val_loss: 0.1185 - val_acc: 0.9500 - 150s/epoch - 2s/step\n",
            "Epoch 18/50\n",
            "100/100 - 150s - loss: 0.2108 - acc: 0.9110 - val_loss: 0.1186 - val_acc: 0.9510 - 150s/epoch - 2s/step\n",
            "Epoch 19/50\n",
            "100/100 - 151s - loss: 0.1998 - acc: 0.9205 - val_loss: 0.1163 - val_acc: 0.9530 - 151s/epoch - 2s/step\n",
            "Epoch 20/50\n",
            "100/100 - 151s - loss: 0.2074 - acc: 0.9050 - val_loss: 0.1157 - val_acc: 0.9530 - 151s/epoch - 2s/step\n",
            "Epoch 21/50\n",
            "100/100 - 150s - loss: 0.1930 - acc: 0.9265 - val_loss: 0.1159 - val_acc: 0.9510 - 150s/epoch - 2s/step\n",
            "Epoch 22/50\n",
            "100/100 - 151s - loss: 0.1992 - acc: 0.9180 - val_loss: 0.1153 - val_acc: 0.9510 - 151s/epoch - 2s/step\n",
            "Epoch 23/50\n",
            "100/100 - 151s - loss: 0.2042 - acc: 0.9095 - val_loss: 0.1139 - val_acc: 0.9540 - 151s/epoch - 2s/step\n",
            "Epoch 24/50\n",
            "100/100 - 152s - loss: 0.2145 - acc: 0.9115 - val_loss: 0.1140 - val_acc: 0.9550 - 152s/epoch - 2s/step\n",
            "Epoch 25/50\n",
            "100/100 - 152s - loss: 0.1958 - acc: 0.9185 - val_loss: 0.1137 - val_acc: 0.9540 - 152s/epoch - 2s/step\n",
            "Epoch 26/50\n",
            "100/100 - 151s - loss: 0.1898 - acc: 0.9165 - val_loss: 0.1123 - val_acc: 0.9540 - 151s/epoch - 2s/step\n",
            "Epoch 27/50\n",
            "100/100 - 150s - loss: 0.1974 - acc: 0.9160 - val_loss: 0.1121 - val_acc: 0.9540 - 150s/epoch - 2s/step\n",
            "Epoch 28/50\n",
            "100/100 - 150s - loss: 0.1882 - acc: 0.9220 - val_loss: 0.1120 - val_acc: 0.9560 - 150s/epoch - 2s/step\n",
            "Epoch 29/50\n",
            "100/100 - 150s - loss: 0.1922 - acc: 0.9180 - val_loss: 0.1120 - val_acc: 0.9560 - 150s/epoch - 2s/step\n",
            "Epoch 30/50\n",
            "100/100 - 150s - loss: 0.2120 - acc: 0.9055 - val_loss: 0.1115 - val_acc: 0.9550 - 150s/epoch - 1s/step\n",
            "Epoch 31/50\n",
            "100/100 - 150s - loss: 0.2084 - acc: 0.9105 - val_loss: 0.1112 - val_acc: 0.9570 - 150s/epoch - 1s/step\n",
            "Epoch 32/50\n",
            "100/100 - 150s - loss: 0.1922 - acc: 0.9205 - val_loss: 0.1112 - val_acc: 0.9570 - 150s/epoch - 1s/step\n",
            "Epoch 33/50\n",
            "100/100 - 150s - loss: 0.1931 - acc: 0.9220 - val_loss: 0.1100 - val_acc: 0.9590 - 150s/epoch - 1s/step\n",
            "Epoch 34/50\n",
            "100/100 - 150s - loss: 0.1854 - acc: 0.9250 - val_loss: 0.1108 - val_acc: 0.9570 - 150s/epoch - 2s/step\n",
            "Epoch 35/50\n",
            "100/100 - 150s - loss: 0.1980 - acc: 0.9205 - val_loss: 0.1101 - val_acc: 0.9570 - 150s/epoch - 2s/step\n",
            "Epoch 36/50\n",
            "100/100 - 150s - loss: 0.1923 - acc: 0.9185 - val_loss: 0.1095 - val_acc: 0.9590 - 150s/epoch - 2s/step\n",
            "Epoch 37/50\n",
            "100/100 - 149s - loss: 0.2037 - acc: 0.9200 - val_loss: 0.1099 - val_acc: 0.9560 - 149s/epoch - 1s/step\n",
            "Epoch 38/50\n",
            "100/100 - 150s - loss: 0.1941 - acc: 0.9180 - val_loss: 0.1092 - val_acc: 0.9590 - 150s/epoch - 1s/step\n",
            "Epoch 39/50\n",
            "100/100 - 150s - loss: 0.1892 - acc: 0.9160 - val_loss: 0.1092 - val_acc: 0.9590 - 150s/epoch - 2s/step\n",
            "Epoch 40/50\n",
            "100/100 - 151s - loss: 0.1934 - acc: 0.9185 - val_loss: 0.1086 - val_acc: 0.9590 - 151s/epoch - 2s/step\n",
            "Epoch 41/50\n"
          ]
        }
      ],
      "source": [
        "# al tener 50 épocas va a demorar en entrenar\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=50,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "He interrumpido el entrenamiento porque me lleva más de media hora, pero haber entrenado 40 épocas me permite saber que he mejorado un poco mi modelo, ahora tendrá un precisión que ronda el 92%, no es mucho más, pero es una leve mejora.\n",
        "\n",
        "Voy a trazar la pérdida y la precisión del entrenamiento y la validación para demostrarlo de manera concluyente:\n",
        "(Realmente dejo el código indicado de la representación, que sería igual a lo hecho en las partes 1 y 2, pero al no haber entrenado el modelo anterior al completo no puedo representarlo graficamente)."
      ],
      "metadata": {
        "id": "aZ_H_9f4p1WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "\n",
        "plt.plot(epochs, acc)\n",
        "plt.plot(epochs, val_acc)\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss)\n",
        "plt.plot(epochs, val_loss)\n",
        "plt.title('Training and validation loss')"
      ],
      "metadata": {
        "id": "t33fbo9QqlMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Parte3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTDCWECrvO9bL7Zss4GTC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}